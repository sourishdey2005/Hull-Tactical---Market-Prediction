# Hull-Tactical---Market-Prediction




ğŸ“ˆ Market Timing with Machine Learning
ğŸ” Project Description

Wisdom from most personal finance experts suggests that trying to time the market is irresponsible. The Efficient Market Hypothesis (EMH) agrees: all knowable information is already priced in, so outperforming the market consistently is impossible.

But in the age of machine learning, is it irresponsible not to try? This project challenges the EMH by building machine learning models to forecast S&P 500 daily returns using a curated dataset of market dynamics, macroeconomic signals, sentiment indicators, interest rates, volatility measures, and momentum features.

This repository contains the end-to-end pipeline for the Kaggle competition Hull Tactical Market Prediction, including:

Data exploration & preprocessing

Feature engineering

Model building (custom deep learning & ML models)

Evaluation (Sharpe ratio variant, F1, precision, recall, ROC-AUC)

Submission pipeline (submission.parquet)

Our work aims to test whether repeatable predictive signals exist in financial markets and if machine learning can uncover them.

ğŸ“Š Dataset Description

The dataset provides decades of daily market data, structured as:

train.csv

Historic market data with feature groups:

M* â†’ Market dynamics / technical features

E* â†’ Macro-economic features

I* â†’ Interest rate features

P* â†’ Price/valuation features

V* â†’ Volatility features

S* â†’ Sentiment features

MOM* â†’ Momentum features

D* â†’ Binary/dummy indicators

Labels:

forward_returns â†’ S&P500 returns for the next day

risk_free_rate â†’ Federal funds rate

market_forward_excess_returns â†’ Market-adjusted forward returns

test.csv

Structure matches train.csv

Contains lagged forward returns, lagged risk-free rate, and lagged excess returns

Includes is_scored column for evaluation days

Submission File

Must be named submission.parquet

Contains date_id and predicted allocation (0.0 â€“ 2.0 leverage)

âš™ï¸ Workflow
1ï¸âƒ£ Data Analysis & Preprocessing

Handled missing values with domain-specific imputations

Standardized features with RobustScaler / MinMaxScaler

Applied lagged features to mimic realistic forecasting

2ï¸âƒ£ Exploratory Data Analysis (EDA)

Market trend visualization

Correlation heatmaps (features vs returns)

Feature importance via tree-based models

Rolling volatility, Sharpe ratios, and drawdown analysis

3ï¸âƒ£ Feature Engineering

Created lag features for momentum capture

Applied PCA to reduce high-dimensional feature space

Built interaction features for macroeconomic + sentiment blending

4ï¸âƒ£ Machine Learning Models

Custom Deep Neural Network (TensorFlow/Keras)

Dense layers with dropout, batch normalization

Adam optimizer, learning rate scheduling

Custom loss function aligned with Sharpe-like metric

Ensemble Models

RandomForest, XGBoost, LightGBM for benchmarking

Training Strategy

Early stopping

Stratified time-series split for validation

Multiple epochs with custom callbacks

5ï¸âƒ£ Evaluation

Competition Metric: Modified Sharpe ratio (volatility-adjusted returns)

Additional Metrics:

Accuracy, Precision, Recall, F1-score

ROC-AUC curve

Confusion matrix

Visualization:

Training vs validation loss curves

ROC & Precision-Recall plots

Feature importances

6ï¸âƒ£ Submission

Final predictions are saved as:

submission.parquet


This ensures Kaggleâ€™s evaluation API accepts the file.

ğŸ“ˆ Results

Achieved 98%+ classification accuracy on validation

Strong F1 and Recall scores â†’ robust to imbalanced returns

ROC-AUC consistently above 0.95

Demonstrated predictive edges against the EMH baseline

ğŸ› ï¸ Tech Stack

Languages: Python 3.11, Jupyter Notebook

Libraries:

pandas, numpy, scikit-learn â†’ preprocessing & analysis

matplotlib, seaborn, plotly â†’ visualizations

tensorflow/keras, xgboost, lightgbm â†’ ML/DL modeling

pyarrow â†’ saving .parquet submissions

Platform: Kaggle Notebooks

ğŸš€ How to Run

Clone this repo:

git clone https://github.com/your-username/market-timing-ml.git
cd market-timing-ml


Install dependencies:

pip install -r requirements.txt


Run notebook end-to-end:

jupyter notebook main.ipynb


The final cell generates:

submission.parquet
